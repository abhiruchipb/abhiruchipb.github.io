# ensemble learning (the more the merrier)

while training an ml model, we all run into the age-old struggle of balancing the bias-variance tradeoff—it’s like trying to eat healthy while also craving dessert.  

one of the techniques used in ml to lower both bias and variance at the same time is **ensemble learning**. 
okay, don’t let the term scare you—it’s actually a super simple concept.  

ensemble learning is based on a very common idea called **wisdom of the crowd**. 
basically, instead of relying on a single model, we bring together multiple weak models (models that perform just a little better than random guessing), and somehow, they combine forces to create a much stronger and more accurate model. 
think of it like a group project where nobody is a genius, but together, they somehow manage to get an a.

ensemble learning is used in ml because:  

1. **better accuracy** – multiple weak models combine to make a stronger, more accurate model. kinda like how a group of average students can somehow ace a project together.  

2. **reduces overfitting** – individual models might overfit to specific data, but when combined, they generalize better. think of it as getting multiple opinions instead of trusting just one person.  

3. **handles bias-variance tradeoff** – it helps balance bias (oversimplification) and variance (overfitting), making the model more reliable. basically, it’s the ml version of "work smarter, not harder."  

4. **more robust predictions** – since it takes input from multiple models, it’s less likely to make extreme mistakes. like how a jury makes better decisions than just one judge.

![image](https://raw.githubusercontent.com/abhiruchipb/abhiruchipb.github.io/refs/heads/main/_posts/images/Screenshot%202025-02-04%20133155.png)

for example, this this image, we combine outputs from three different models to get a more accurate model.

## voting regressor and voting classifier

**voting classifier** and **voting regressor** are ensemble learning techniques where multiple models "vote" to make a final prediction. By combining different models, the overall prediction is more reliable and robust.

# 1. voting classifier

In a **voting classifier**, each individual classifier in the ensemble predicts a class label, and the final prediction is made based on the majority vote (for classification tasks). 

there are two types of voting:

- **hard voting**: each model in the ensemble predicts a class label. The final prediction is the class that receives the majority of votes.
  for example, if three classifiers predict `A`, `B`, and `A`, the final prediction is `A`.
  
- **soft voting**: each model predicts probabilities for each class. The final class is the one with the highest average probability. this method works best when the classifiers provide well-calibrated probabilities.
    for example, if the three classifiers predict class probabilities as follows:
    - class A: 0.6, 0.3, 0.8
    - class B: 0.4, 0.7, 0.2
    
  the final prediction will be class A, as it has the highest average probability (0.6 + 0.3 + 0.8 = 1.7).

example: voting classifier (hard voting)

```python
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Load dataset
data = load_iris()
X = data.data
y = data.target

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create individual classifiers
clf1 = LogisticRegression(max_iter=200)
clf2 = SVC(probability=True)
clf3 = DecisionTreeClassifier()

# Create a voting classifier (hard voting)
voting_clf = VotingClassifier(estimators=[('lr', clf1), ('svc', clf2), ('dt', clf3)], voting='hard')

# Train the model
voting_clf.fit(X_train, y_train)

# Evaluate the model
accuracy = voting_clf.score(X_test, y_test)
print(f"Voting Classifier Accuracy: {accuracy:.2f}")
```

# 2. voting regressor

in a **voting regressor**, the ensemble predicts continuous values. instead of selecting the majority vote, the final prediction is made by averaging the predictions of all the individual regressors.

example: voting regressor

```python
from sklearn.ensemble import VotingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# Generate regression data
X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=42)

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create individual regressors
reg1 = LinearRegression()
reg2 = DecisionTreeRegressor(random_state=42)
reg3 = SVR()

# Create a voting regressor
voting_reg = VotingRegressor(estimators=[('lr', reg1), ('dt', reg2), ('svr', reg3)])

# Train the model
voting_reg.fit(X_train, y_train)

# Evaluate the model
score = voting_reg.score(X_test, y_test)
print(f"Voting Regressor R-squared: {score:.2f}")
```
now, lets look at some ensemble techniques:

## 1. bagging

when we want to improve the performance of our machine learning models, we often look to ensemble techniques. bagging, pasting, random patches, and subspaces are all variations of ensemble learning that use multiple models to make better predictions. let’s break them down in simple terms.

# bagging (bootstrap aggregating)

bagging stands for **bootstrap aggregating**, which is a technique where we train multiple models on different random samples of the data (with replacement) and then combine their predictions. the idea is that each model will see a slightly different view of the data, and combining their predictions will make the final model more robust.

**how it works**:  
- random samples of the data (with replacement) are used to train different models. 
- the final prediction is typically made by averaging the predictions for regression or taking a majority vote for classification.

### example: bagging with decision trees

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# load dataset
data = load_iris()
X = data.data
y = data.target

# split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# create a base classifier
base_clf = DecisionTreeClassifier()

# create a bagging classifier
bagging_clf = BaggingClassifier(base_clf, n_estimators=50)

# train the model
bagging_clf.fit(X_train, y_train)

# evaluate the model
accuracy = bagging_clf.score(X_test, y_test)
print(f"bagging classifier accuracy: {accuracy:.2f}")
```
# pasting
pasting is similar to bagging but with one key difference: no replacement. in pasting, each model is trained on a random sample of the data, but once a sample is used, it’s not reused in other models. this means that each model gets a different subset of the data.

**how it works**:  
- random samples of the data (without replacement) are used to train different models.
- the final prediction is averaged for regression or determined by majority vote for classification.

example: pasting with decision trees

```
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# create a bagging classifier with pasting (no replacement)
pasting_clf = BaggingClassifier(base_clf, n_estimators=50, bootstrap=False)

# train the model
pasting_clf.fit(X_train, y_train)

# evaluate the model
accuracy = pasting_clf.score(X_test, y_test)
print(f"pasting classifier accuracy: {accuracy:.2f}")
```

# random patches
random patches is a variation of bagging where we not only take random samples of the data, but we also take random subsets of features (i.e., only some features are used for training). this can be useful when working with high-dimensional data, where it’s expensive to use all features for each model.

**how it works:**
- random samples of the data (with or without replacement) and random subsets of features are used to train each model.
- the final prediction is averaged or decided by majority vote.

example: random patches with decision trees
```
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# create a bagging classifier with random patches (random subsets of features)
random_patches_clf = BaggingClassifier(base_clf, n_estimators=50, max_features=0.5)

# train the model
random_patches_clf.fit(X_train, y_train)

# evaluate the model
accuracy = random_patches_clf.score(X_test, y_test)
print(f"random patches classifier accuracy: {accuracy:.2f}")
```

# subspaces
subspaces are similar to random patches, but they specifically focus on the random subset of features used to train each model. subspaces can be helpful in reducing overfitting when dealing with many features.

**how it works:**
- a random subset of features is selected for each model, and this helps to reduce correlation between the models.
- the final prediction is made by averaging or majority vote.

example: subspaces with decision trees
```
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# create a bagging classifier with subspaces (random subset of features)
subspaces_clf = BaggingClassifier(base_clf, n_estimators=50, max_features=0.5)

# train the model
subspaces_clf.fit(X_train, y_train)

# evaluate the model
accuracy = subspaces_clf.score(X_test, y_test)
print(f"subspaces classifier accuracy: {accuracy:.2f}")
```

# gridsearchcv and randomsearchcv
these are methods used to find the best hyperparameters for a model. both are used for hyperparameter tuning, but they work in slightly different ways.

## gridsearchcv
gridsearchcv is an exhaustive search technique where we define a grid of hyperparameters, and gridsearch will try all possible combinations of those hyperparameters to find the best one. it’s like trying every possible combination of ingredients in a recipe to find the best flavor.

**how it works:**
- you define a grid of hyperparameters.
- gridsearch tries every combination of those hyperparameters.
- the model is evaluated for each combination, and the best one is selected.

example: gridsearchcv
```
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# define the model
model = RandomForestClassifier()

# define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [10, 20, 30]
}

# create the grid search object
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)

# fit the model
grid_search.fit(X_train, y_train)

# get the best hyperparameters
print("best parameters:", grid_search.best_params_)
```

## randomsearchcv
randomsearchcv is a more efficient method because it randomly selects hyperparameters from a defined search space. instead of trying every possible combination like gridsearch, randomsearch tries a random subset of combinations, making it faster especially for large search spaces.

**how it works:**
- you define a range of hyperparameters.
- randomsearch randomly selects combinations of hyperparameters and evaluates them.
- it stops after a predefined number of combinations have been tested.

example: randomsearchcv

```
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint

# define the model
model = RandomForestClassifier()

# define the parameter distribution
param_dist = {
    'n_estimators': randint(50, 200),
    'max_depth': randint(10, 30)
}

# create the random search object
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=100, cv=5)

# fit the model
random_search.fit(X_train, y_train)

# get the best hyperparameters
print("best parameters:", random_search.best_params_)
```
