# ensemble learning (the more the merrier)

while training an ml model, we all run into the age-old struggle of balancing the bias-variance tradeoff—it’s like trying to eat healthy while also craving dessert.  

one of the techniques used in ml to lower both bias and variance at the same time is **ensemble learning**. 
okay, don’t let the term scare you—it’s actually a super simple concept.  

ensemble learning is based on a very common idea called **wisdom of the crowd**. 
basically, instead of relying on a single model, we bring together multiple weak models (models that perform just a little better than random guessing), and somehow, they combine forces to create a much stronger and more accurate model. 
think of it like a group project where nobody is a genius, but together, they somehow manage to get an a.

ensemble learning is used in ml because:  

1. **better accuracy** – multiple weak models combine to make a stronger, more accurate model. kinda like how a group of average students can somehow ace a project together.  

2. **reduces overfitting** – individual models might overfit to specific data, but when combined, they generalize better. think of it as getting multiple opinions instead of trusting just one person.  

3. **handles bias-variance tradeoff** – it helps balance bias (oversimplification) and variance (overfitting), making the model more reliable. basically, it’s the ml version of "work smarter, not harder."  

4. **more robust predictions** – since it takes input from multiple models, it’s less likely to make extreme mistakes. like how a jury makes better decisions than just one judge.

![image](https://raw.githubusercontent.com/abhiruchipb/abhiruchipb.github.io/refs/heads/main/_posts/images/Screenshot%202025-02-04%20133155.png)

for example, this this image, we combine outputs from three different models to get a more accurate model.

## ovting regressor and voting classifier

**voting classifier** and **voting regressor** are ensemble learning techniques where multiple models "vote" to make a final prediction. By combining different models, the overall prediction is more reliable and robust.

# 1. voting classifier

In a **voting classifier**, each individual classifier in the ensemble predicts a class label, and the final prediction is made based on the majority vote (for classification tasks). 

there are two types of voting:

- **hard voting**: each model in the ensemble predicts a class label. The final prediction is the class that receives the majority of votes.
  for example, if three classifiers predict `A`, `B`, and `A`, the final prediction is `A`.
  
- **soft voting**: each model predicts probabilities for each class. The final class is the one with the highest average probability. this method works best when the classifiers provide well-calibrated probabilities.
    for example, if the three classifiers predict class probabilities as follows:
    - class A: 0.6, 0.3, 0.8
    - class B: 0.4, 0.7, 0.2
    
  the final prediction will be class A, as it has the highest average probability (0.6 + 0.3 + 0.8 = 1.7).

### example: voting classifier (hard voting)

```python
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Load dataset
data = load_iris()
X = data.data
y = data.target

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create individual classifiers
clf1 = LogisticRegression(max_iter=200)
clf2 = SVC(probability=True)
clf3 = DecisionTreeClassifier()

# Create a voting classifier (hard voting)
voting_clf = VotingClassifier(estimators=[('lr', clf1), ('svc', clf2), ('dt', clf3)], voting='hard')

# Train the model
voting_clf.fit(X_train, y_train)

# Evaluate the model
accuracy = voting_clf.score(X_test, y_test)
print(f"Voting Classifier Accuracy: {accuracy:.2f}")
```

# 2. voting regressor

in a **voting regressor**, the ensemble predicts continuous values. instead of selecting the majority vote, the final prediction is made by averaging the predictions of all the individual regressors.

### example: voting regressor

```python
from sklearn.ensemble import VotingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# Generate regression data
X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=42)

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create individual regressors
reg1 = LinearRegression()
reg2 = DecisionTreeRegressor(random_state=42)
reg3 = SVR()

# Create a voting regressor
voting_reg = VotingRegressor(estimators=[('lr', reg1), ('dt', reg2), ('svr', reg3)])

# Train the model
voting_reg.fit(X_train, y_train)

# Evaluate the model
score = voting_reg.score(X_test, y_test)
print(f"Voting Regressor R-squared: {score:.2f}")
```
