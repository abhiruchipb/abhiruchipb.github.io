## Part II: brains to bytes - journey from neurons to perceptrons

# inspo from biological neurons
imagine your brain as a bustling metropolis, with trillions of neurons playing the role of its inhabitants. <br>
these neurons are the ultimate multitaskers, constantly processing information, making decisions, and passing on messages.

hereâ€™s the deal:

***dendrites*** are like antennas, receiving signals from other neurons. 

***synapses*** are the connection points where neurons â€œtalkâ€ to each other. 

***soma*** processes the incoming signals, like a central office handling information. 

***axons*** transmit the processed signal to the next set of neurons.

![Diagram](https://miro.medium.com/v2/resize:fit:1400/1*K1ee1SzB0lxjIIo7CGI7LQ.png)

this interconnected network forms a hierarchy. at the base, neurons might detect simple edges or corners (think: your brain spotting the outline of a cat). <br>
higher up, they start combining this information to recognize patterns or even, say, a fluffy tail.

# meet the mcculloch pitts neuron - the first artificial brain cell
in 1943, warren mcculloch and walter pitts proposed a simplified computational model of a neuron. <br>
called the mcculloch pitts (MP) Neuron which laid the groundwork for modern neural networks.

the MP neuron isnâ€™t as fancy as its biological counterpart, but it does the job. 

hereâ€™s how it works:

- it takes a bunch of binary inputs (0s and 1s) from its environment. <br>
- each input can either excite or inhibit the neuron. <br>
- it aggregates these inputs using a function ğ‘”(ğ‘¥), which is essentially the sum of the inputs. <br>
- it compares this sum to a threshold ğœƒ.<br>
- if ğ‘”(ğ‘¥) >= ğœƒ, the neuron â€œfiresâ€ (output = 1).<br>
  otherwise, it stays quiet (output = 0).<br>


![diagram](https://miro.medium.com/v2/resize:fit:738/1*fDHlg9iNo0LLK4czQqqO9A.png)

![image](https://raw.githubusercontent.com/abhiruchipb/abhiruchipb.github.io/refs/heads/main/_posts/images/image4.png)

for example, if we fix a threshold ğœƒ = 3, the mp neuron fires only when atleast 3 of the xi's are 1.

this process is called ***thresholding logic***, and itâ€™s surprisingly powerful.

# boolean Functions and the MP Neuron

an MP neuron has two main parts:
- ***aggregation:*** the neuron sums its inputs, optionally weighted.<br>
- ***thresholding:*** it outputs 1 if the sum exceeds a threshold value (Î¸), otherwise 0.<br>

mathematically:
![image](https://raw.githubusercontent.com/abhiruchipb/abhiruchipb.github.io/refs/heads/main/_posts/Screenshot%202024-12-31%20020838.png)

Here:
ğ‘¥ğ‘– : input values (0 or 1 for Boolean functions) <br>
ğ‘¤ğ‘– : weights (can be binary or real numbers) <br>
ğœƒ : threshold <br>

**Can a Single MP Neuron Do It All?**
the mcculloch-pitts (MP) neuron is a fundamental computational model of a neuron. its simplicity allows us to explore basic Boolean functions and understand the limits of single-layer neural networks.

1. **AND Function**
- the neuron fires only if all inputs are 1. <br>
- Parameters:<br>
  - Weights (ğ‘¤1, ğ‘¤2,â€¦): 1, 1,â€¦ (all equal to 1).<br>
  - Threshold (ğœƒ): Number of inputs, ğ‘›.<br>
- Why? the total sum of inputs needs to match ğ‘› to output 1. <br>
- E.g. ğ‘¥1 + ğ‘¥2 = 2 when ğ‘› = 2 <br>
- Number of Parameters: ğ‘› weights + 1 threshold = ğ‘› + 1
