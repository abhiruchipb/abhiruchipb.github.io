## Part II: brains to bytes - journey from neurons to perceptrons

# inspo from biological neurons
imagine your brain as a bustling metropolis, with trillions of neurons playing the role of its inhabitants. <br>
these neurons are the ultimate multitaskers, constantly processing information, making decisions, and passing on messages.

here’s the deal:

***dendrites*** are like antennas, receiving signals from other neurons. 

***synapses*** are the connection points where neurons “talk” to each other. 

***soma*** processes the incoming signals, like a central office handling information. 

***axons*** transmit the processed signal to the next set of neurons.

![Diagram](https://miro.medium.com/v2/resize:fit:1400/1*K1ee1SzB0lxjIIo7CGI7LQ.png)

this interconnected network forms a hierarchy. at the base, neurons might detect simple edges or corners (think: your brain spotting the outline of a cat). <br>
higher up, they start combining this information to recognize patterns or even, say, a fluffy tail.

# meet the mcculloch pitts neuron - the first artificial brain cell
in 1943, warren mcculloch and walter pitts proposed a simplified computational model of a neuron. <br>
called the mcculloch pitts (MP) Neuron which laid the groundwork for modern neural networks.

the MP neuron isn’t as fancy as its biological counterpart, but it does the job. 

here’s how it works:

- it takes a bunch of binary inputs (0s and 1s) from its environment. <br>
- each input can either excite or inhibit the neuron. <br>
- it aggregates these inputs using a function 𝑔(𝑥), which is essentially the sum of the inputs. <br>
- it compares this sum to a threshold 𝜃.<br>
- if 𝑔(𝑥) >= 𝜃, the neuron “fires” (output = 1).<br>
  otherwise, it stays quiet (output = 0).<br>


![diagram](https://miro.medium.com/v2/resize:fit:738/1*fDHlg9iNo0LLK4czQqqO9A.png)

![image](https://raw.githubusercontent.com/abhiruchipb/abhiruchipb.github.io/refs/heads/main/_posts/images/image4.png)

for example, if we fix a threshold 𝜃 = 3, the mp neuron fires only when atleast 3 of the xi's are 1.

this process is called ***thresholding logic***, and it’s surprisingly powerful.

# boolean Functions and the MP Neuron

an MP neuron has two main parts:
- ***aggregation:*** the neuron sums its inputs, optionally weighted.<br>
- ***thresholding:*** it outputs 1 if the sum exceeds a threshold value (θ), otherwise 0.<br>

mathematically:
![image](https://raw.githubusercontent.com/abhiruchipb/abhiruchipb.github.io/refs/heads/main/_posts/Screenshot%202024-12-31%20020838.png)

Here:
𝑥𝑖 : input values (0 or 1 for Boolean functions) <br>
𝑤𝑖 : weights (can be binary or real numbers) <br>
𝜃 : threshold <br>

**Can a Single MP Neuron Do It All?**
the mcculloch-pitts (MP) neuron is a fundamental computational model of a neuron. its simplicity allows us to explore basic Boolean functions and understand the limits of single-layer neural networks.

1. **AND Function**
- the neuron fires only if all inputs are 1. <br>
- Parameters:<br>
  - Weights (𝑤1, 𝑤2,…): 1, 1,… (all equal to 1).<br>
  - Threshold (𝜃): Number of inputs, 𝑛.<br>
- Why? the total sum of inputs needs to match 𝑛 to output 1. <br>
- E.g. 𝑥1 + 𝑥2 = 2 when 𝑛 = 2 <br>
- Number of Parameters: 𝑛 weights + 1 threshold = 𝑛 + 1
