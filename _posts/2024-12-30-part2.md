## Part II: brains to bytes - journey from neurons to perceptrons

# inspo from biological neurons
imagine your brain as a bustling metropolis, with trillions of neurons playing the role of its inhabitants. <br>
these neurons are the ultimate multitaskers, constantly processing information, making decisions, and passing on messages.

hereâ€™s the deal:

***dendrites*** are like antennas, receiving signals from other neurons. 

***synapses*** are the connection points where neurons â€œtalkâ€ to each other. 

***soma*** processes the incoming signals, like a central office handling information. 

***axons*** transmit the processed signal to the next set of neurons.

![Diagram](https://miro.medium.com/v2/resize:fit:1400/1*K1ee1SzB0lxjIIo7CGI7LQ.png)

this interconnected network forms a hierarchy. at the base, neurons might detect simple edges or corners (think: your brain spotting the outline of a cat). <br>
higher up, they start combining this information to recognize patterns or even, say, a fluffy tail.

# meet the mcculloch pitts neuron - the first artificial brain cell
in 1943, warren mcculloch and walter pitts proposed a simplified computational model of a neuron. <br>
called the mcculloch pitts (MP) Neuron which laid the groundwork for modern neural networks.

the MP neuron isnâ€™t as fancy as its biological counterpart, but it does the job. 

hereâ€™s how it works:

- it takes a bunch of binary inputs (0s and 1s) from its environment. <br>
- each input can either excite or inhibit the neuron. <br>
- it aggregates these inputs using a function ğ‘”(ğ‘¥), which is essentially the sum of the inputs. <br>
- it compares this sum to a threshold ğœƒ.<br>
- if ğ‘”(ğ‘¥) >= ğœƒ, the neuron â€œfiresâ€ (output = 1).<br>
  otherwise, it stays quiet (output = 0).<br>


![diagram](https://miro.medium.com/v2/resize:fit:738/1*fDHlg9iNo0LLK4czQqqO9A.png)

![image](https://raw.githubusercontent.com/abhiruchipb/abhiruchipb.github.io/refs/heads/main/_posts/images/image4.png)

for example, if we fix a threshold ğœƒ = 3, the mp neuron fires only when atleast 3 of the xi's are 1.

this process is called ***thresholding logic***, and itâ€™s surprisingly powerful.

# boolean Functions and the MP Neuron

an MP neuron has two main parts:
- ***aggregation:*** the neuron sums its inputs, optionally weighted.<br>
- ***thresholding:*** it outputs 1 if the sum exceeds a threshold value (Î¸), otherwise 0.<br>

mathematically:
![image](https://raw.githubusercontent.com/abhiruchipb/abhiruchipb.github.io/refs/heads/main/_posts/Screenshot%202024-12-31%20020838.png)

Here:
ğ‘¥ğ‘– : input values (0 or 1 for Boolean functions) <br>
ğ‘¤ğ‘– : weights (can be binary or real numbers) <br>
ğœƒ : threshold <br>

**can a single mp neuron do it all?**  
the mcculloch-pitts (mp) neuron is a fundamental computational model of a neuron. its simplicity allows us to explore basic boolean functions and understand the limits of single-layer neural networks.

1. **and function**  
- the neuron fires only if all inputs are 1. <br>  
- parameters:<br>  
  - weights (ğ‘¤1, ğ‘¤2,â€¦): 1, 1,â€¦ (all equal to 1).<br>  
  - threshold (ğœƒ): number of inputs, ğ‘›.<br>  
- why? the total sum of inputs needs to match ğ‘› to output 1. <br>  
- e.g. neuron fires only when ğ‘¥1 + ğ‘¥2 + ğ‘¥3 = 3 because ğ‘› = 3. <br>  
![image](https://raw.githubusercontent.com/abhiruchipb/abhiruchipb.github.io/refs/heads/main/_posts/images/and.webp)

2. **or function**  
- the neuron fires if at least one input is 1. <br>  
- parameters:<br>  
  - weights (ğ‘¤1, ğ‘¤2,â€¦): 1, 1,â€¦ (all equal to 1).<br>  
  - threshold (ğœƒ): 1.<br>  
- why? a single input firing (sum â‰¥ 1) is enough to cross the threshold. <br>  
- e.g. for 3 inputs, the neuron fires if ğ‘¥1 + ğ‘¥2 + ğ‘¥3 â‰¥ 1.  
  if either ğ‘¥1 = 1 or ğ‘¥2 = 1 or ğ‘¥3 = 1 (or all), the output is 1.<br>  
![image](https://raw.githubusercontent.com/abhiruchipb/abhiruchipb.github.io/refs/heads/main/_posts/images/or.webp)

3. **not function**  
- the neuron flips the input: outputs 1 if the input is 0, and 0 if the input is 1. <br>  
- parameters:<br>  
  - weight (ğ‘¤): -1.<br>  
  - threshold (ğœƒ): 0.<br>  
- why? if the input ğ‘¥ = 0, the sum (ğ‘¤ğ‘¥) = 0, which meets or exceeds the threshold, so the output is 1. if ğ‘¥ = 1, the sum (ğ‘¤ğ‘¥) = -1, which is below the threshold, so the output is 0.<br> ![image](https://raw.githubusercontent.com/abhiruchipb/abhiruchipb.github.io/refs/heads/main/_posts/images/not.webp)  

4. **nor function**  
- the neuron outputs 1 only if all inputs are 0. <br>  
- parameters:<br>  
  - weights (ğ‘¤1, ğ‘¤2,â€¦): -1, -1,â€¦ (all equal to -1).<br>  
  - threshold (ğœƒ): 0.<br>  
- why? if all inputs ğ‘¥1, ğ‘¥2,â€¦ are 0, the sum (ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 + â€¦) = 0, which meets or exceeds the threshold, so the output is 1.
if any input is 1, the sum becomes less than 0, so the output is 0.<br>  
![image](https://raw.githubusercontent.com/abhiruchipb/abhiruchipb.github.io/refs/heads/main/_posts/images/nor.webp)

# the perceptron

the **perceptron**, introduced by frank rosenblatt in 1958, is a foundational algorithm in machine learning used for binary classification. it improves upon the mcculloch-pitts neuron by introducing **numerical weights** for inputs and a mechanism to learn these weights from data. here's a concise yet detailed explanation:

### key concepts:

1. **inputs and weights**:
    - inputs (x1, x2, ..., xn) can be any real-valued numbers (not just binary).
    - each input is assigned a weight (w1, w2, ..., wn), representing its importance.
    - a **bias term** (w0) is added to shift the decision boundary, acting as a threshold or prior preference.

2. **activation function**:
    - the perceptron computes a weighted sum: z = Î£(from i=1 to n) wi xi + w0.  (Note: Î£ represents summation)
    - it uses a **step function** to output:
    ```
    y = 1 if z >= Î¸
    y = 0 otherwise
    ```
    - here, Î¸ is the threshold (often set to 0).
![image]([https://raw.githubusercontent.com/abhiruchipb/abhiruchipb.github.io/refs/heads/main/_posts/images/nor.webp](https://raw.githubusercontent.com/abhiruchipb/abhiruchipb.github.io/refs/heads/main/_posts/images/Screenshot%202025-01-30%20015257.png))


3. **learning**:
    - the weights and bias are learned from training data.
    - the perceptron adjusts weights iteratively to minimize classification errors.

### example: movie preference

suppose you want to predict whether you'll like a movie based on three binary inputs:

1. **isdirectornolan**: 1 if directed by nolan, else 0.
2. **isactormattdamon**: 1 if starring matt damon, else 0.
3. **isgenrethriller**: 1 if the genre is thriller, else 0.

based on past data, you might assign:
- w1 = 3.0 (high weight for nolan),
- w2 = 1.0 (moderate weight for matt damon),
- w3 = 1.0 (moderate weight for thriller),
- w0 = -2.0 (bias, reflecting a moderate threshold).

for a movie with:
- isdirectornolan = 1,
- isactormattdamon = 0,
- isgenrethriller = 0,

the weighted sum is:
z = (3.0 * 1) + (1.0 * 0) + (1.0 * 0) + (-2.0) = 1.0

if Î¸ = 0, the perceptron outputs 1, meaning you would like the movie. the high weight for nolan allows the perceptron to cross the threshold even if other inputs are unfavorable.

### limitations:

1. **linear separability**: the perceptron can only classify data that is linearly separable.
2. **single-layer**: it cannot model complex, non-linear relationships, which led to the development of multi-layer perceptrons (mlps) and deep learning.
