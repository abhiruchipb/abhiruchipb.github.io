# implemnting all ML algos from scratch (without sklean)
machine learning models often rely on libraries like sklearn, but understanding the math behind them is essential. in this blog, let's build ML algos from scratch in python without using sklearn.
## simple linear regression

simple linear regression is a method to model the relationship between two variables using a straight line. the equation is:

```
y = m * x + b
```

where:
- `m` is the slope (how much y changes for a unit change in x)
- `b` is the intercept (value of y when x is 0)

### mathematical intuition

the goal of linear regression is to find the best-fitting line for the given data points by minimizing the sum of squared errors. this means we try to find values of `m` and `b` that minimize the mean squared error:

```
mse = (1/n) * sum((y_true - y_predicted) ** 2)
```

using calculus, we derive the formulas for `m` and `b` using the least squares method:

```
m = sum((x - mean_x) * (y - mean_y)) / sum((x - mean_x) ** 2)
b = mean_y - m * mean_x
```

this formula ensures that the line minimizes the total squared error between predicted and actual values.

### implementing in python

hereâ€™s the complete code:

```python
import numpy as np

class SimpleLinearRegression:
    def __init__(self):
        self.m = 0  # slope of the line
        self.b = 0  # y-intercept of the line

    def fit(self, X, Y):
        # calculate the means of X and Y
        X_mean, Y_mean = np.mean(X), np.mean(Y)
        
        # calculate the slope (m) using the least squares method
        numerator = sum((X - X_mean) * (Y - Y_mean))
        denominator = sum((X - X_mean) ** 2)
        self.m = numerator / denominator
        
        # calculate the intercept (b)
        self.b = Y_mean - self.m * X_mean

    def predict(self, X):
        # apply the linear equation to predict y values
        return self.m * X + self.b

    def mean_squared_error(self, Y_true, Y_pred):
        # calculate the mean squared error for performance evaluation
        return np.mean((Y_true - Y_pred) ** 2)

# example usage
X = np.array([1, 2, 3, 4, 5])  # input features
Y = np.array([2, 4, 5, 4, 5])  # target values

model = SimpleLinearRegression()
model.fit(X, Y)  # train the model
predictions = model.predict(X)  # generate predictions

print(f"slope (m): {model.m:.3f}, intercept (b): {model.b:.3f}")
print(f"mean squared error: {model.mean_squared_error(Y, predictions):.3f}")
```

